{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import collections\n",
    "from math import log\n",
    "import sys\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Python 3 backwards compatibility tricks\n",
    "if sys.version_info.major > 2:\n",
    "\n",
    "    def xrange(*args, **kwargs):\n",
    "        return iter(range(*args, **kwargs))\n",
    "\n",
    "    def unicode(*args, **kwargs):\n",
    "        return str(*args, **kwargs)\n",
    "\n",
    "class LangModel:\n",
    "    def fit_corpus(self, corpus):\n",
    "        \"\"\"Learn the language model for the whole corpus.\n",
    "\n",
    "        The corpus consists of a list of sentences.\"\"\"\n",
    "        for s in corpus:\n",
    "            self.fit_sentence(s)\n",
    "            \n",
    "       # a = Counter(self.model.values())\n",
    "        #a = sorted(a.items(), key=lambda x: x[1])\n",
    "        #print(\"words count occurance\",a)\n",
    "        vo = self.vocab()\n",
    "        poplist = list()\n",
    "        for w in vo:\n",
    "            if self.model[w] == 1:\n",
    "                poplist.append(w)\n",
    "        self.model[('UNK',)] = len(poplist)\n",
    "        print(\"total UNK is \", len(poplist))\n",
    "        for p in poplist:\n",
    "            self.model.pop(p)\n",
    "        self.norm()\n",
    "    def perplexity(self, corpus):\n",
    "        \"\"\"Computes the perplexity of the corpus by the model.\n",
    "\n",
    "        Assumes the model uses an EOS symbol at the end of each sentence.\n",
    "        \"\"\"\n",
    "        return pow(2.0, self.entropy(corpus))\n",
    "\n",
    "    def entropy(self, corpus):\n",
    "        num_words = 0.0\n",
    "        sum_logprob = 0.0\n",
    "        for s in corpus:\n",
    "            num_words += len(s) + 1 # for EOS\n",
    "            sum_logprob += self.logprob_sentence(s)\n",
    "        return -(1.0/num_words)*(sum_logprob)\n",
    "\n",
    "    def logprob_sentence(self, sentence):\n",
    "        p = 0.0\n",
    "        unigrams = list(ngrams(sentence, 1, pad_left = False, pad_right=False))\n",
    "        for i in xrange(len(sentence)):\n",
    "            p += self.cond_logprob(unigrams[i],unigrams[:i])\n",
    "        #p += self.cond_logprob('END_OF_SENTENCE', unigrams)\n",
    "        return p\n",
    "\n",
    "    # required, update the model when a sentence is observed\n",
    "    def fit_sentence(self, sentence): pass\n",
    "    # optional, if there are any post-training steps (such as normalizing probabilities)\n",
    "    def norm(self): pass\n",
    "    # required, return the log2 of the conditional prob of word, given previous words\n",
    "    def cond_logprob(self, word, previous): pass\n",
    "    # required, the list of words the language model suports (including EOS)\n",
    "    def vocab(self): pass\n",
    "\n",
    "class Trigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001):\n",
    "        self.modeltri = dict()\n",
    "        self.modelbi = dict()\n",
    "        self.model = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        \n",
    "    def inc_word(self, w):\n",
    "        opt = len(w)\n",
    "        if opt == 1:\n",
    "            if w in self.model:\n",
    "                self.model[w] += 1.0\n",
    "            else:\n",
    "                self.model[w] = 1.0\n",
    "        elif opt == 2:\n",
    "            if w in self.modelbi:\n",
    "                self.modelbi[w] += 1.0\n",
    "            else:\n",
    "                self.modelbi[w] = 1.0\n",
    "        elif opt == 3:\n",
    "            if w in self.modeltri:\n",
    "                self.modeltri[w] += 1.0\n",
    "            else:\n",
    "                self.modeltri[w] = 1.0\n",
    "        \n",
    "        \n",
    "    def fit_sentence(self, sentence):\n",
    "        unigrams = list(ngrams(sentence, 1, pad_left = False, pad_right=False))\n",
    "        bigrams = list(ngrams(sentence, 2, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = list(ngrams(sentence, 3, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = trigrams[:len(trigrams)-1]\n",
    "        for w in unigrams:\n",
    "            self.inc_word(w)\n",
    "        for w in bigrams:\n",
    "            self.inc_word(w)\n",
    "        for w in trigrams:\n",
    "            self.inc_word(w)\n",
    "        \n",
    "    def norm(self):\n",
    "        \"\"\"Normalize and convert to log2-probs.\"\"\"\n",
    "        tot = 0.0\n",
    "        for word in self.model:\n",
    "            tot += self.model[word]\n",
    "        ltot = log(tot, 2)\n",
    "        for word in self.model:\n",
    "            self.model[word] = log(self.model[word], 2) - ltot\n",
    "\n",
    "    def cond_logprob(self, word, previous):\n",
    "        if word in self.model:\n",
    "            return self.model[word]\n",
    "        else:\n",
    "            return self.model[('UNK',)]\n",
    "\n",
    "    def vocab(self):\n",
    "        return self.model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram with laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import collections\n",
    "from math import log\n",
    "import sys\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Python 3 backwards compatibility tricks\n",
    "if sys.version_info.major > 2:\n",
    "\n",
    "    def xrange(*args, **kwargs):\n",
    "        return iter(range(*args, **kwargs))\n",
    "\n",
    "    def unicode(*args, **kwargs):\n",
    "        return str(*args, **kwargs)\n",
    "\n",
    "class LangModel:\n",
    "    def fit_corpus(self, corpus):\n",
    "        \"\"\"Learn the language model for the whole corpus.\n",
    "\n",
    "        The corpus consists of a list of sentences.\"\"\"\n",
    "        for s in corpus:\n",
    "            self.fit_sentence(s)\n",
    "         \n",
    "        vo = self.model.keys()\n",
    "        vobi = self.modelbi.keys()\n",
    "        votri = self.modeltri.keys()\n",
    "        for w in vo:\n",
    "            if self.model[w] == 1:\n",
    "                self.poplist.append(w[0])\n",
    "                \n",
    "        self.fimodel[('UNK',)] = len(self.poplist)\n",
    "        print(\"total UNK is \", len(self.poplist))\n",
    "        stop = 0\n",
    "        for tri in votri:\n",
    "            #stop += 1\n",
    "            #if stop == 10000:\n",
    "             #   ('a','b')[0] = 'e'\n",
    "            old = tri\n",
    "            tri = list(tri)\n",
    "            for i in range(0,len(tri)):\n",
    "                if tri[i] in self.poplist:\n",
    "                    tri[i] = 'UNK'\n",
    "            tri = tuple(tri)\n",
    "            if tri in self.fimodel:\n",
    "                    self.fimodel[tri] += self.modeltri[old]\n",
    "            else:\n",
    "                    self.fimodel[tri] = self.modeltri[old]\n",
    "        print(\"finished processing tri with unk\")    \n",
    "        for bi in vobi:\n",
    "            oldbi = bi\n",
    "            bi = list(bi)\n",
    "            for i in range(0,len(bi)):\n",
    "                if bi[i] in self.poplist:\n",
    "                    bi[i] = 'UNK'\n",
    "            bi = tuple(bi)\n",
    "            if bi in self.fimodelbi:\n",
    "                self.fimodelbi[bi] += self.modelbi[oldbi]\n",
    "            else:\n",
    "                self.fimodelbi[bi] = self.modelbi[oldbi]   \n",
    "        print(\"finished processing bi with unk\")    \n",
    "        \n",
    "        self.norm(corpus)\n",
    "\n",
    "    def perplexity(self, corpus):\n",
    "        \"\"\"Computes the perplexity of the corpus by the model.\n",
    "\n",
    "        Assumes the model uses an EOS symbol at the end of each sentence.\n",
    "        \"\"\"\n",
    "        print('---------',self.pro,'------------')\n",
    "        return pow(2.0, self.entropy(corpus))\n",
    "\n",
    "    def entropy(self, corpus):\n",
    "        num_words = 0.0\n",
    "        sum_logprob = 0.0\n",
    "        for s in corpus:\n",
    "            num_words += len(s)  # for EOS\n",
    "            sum_logprob += self.logprob_sentence(s,corpus)\n",
    "        return -(1.0/num_words)*(sum_logprob)\n",
    "\n",
    "    def logprob_sentence(self, sentence,corpus):\n",
    "        p = 0.0\n",
    "        trigrams = list(ngrams(sentence, 3, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = trigrams[:len(trigrams)-1]\n",
    "        for i in xrange(len(sentence)):\n",
    "            p += self.cond_logprob(trigrams[i],trigrams[:i])\n",
    "        return p\n",
    "\n",
    "    # required, update the model when a sentence is observed\n",
    "    def fit_sentence(self, sentence): pass\n",
    "    # optional, if there are any post-training steps (such as normalizing probabilities)\n",
    "    def norm(self,corpus): pass\n",
    "    # required, return the log2 of the conditional prob of word, given previous words\n",
    "    def cond_logprob(self, word, previous): pass\n",
    "    # required, the list of words the language model suports (including EOS)\n",
    "    def vocab(self): pass\n",
    "\n",
    "class Trigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001):\n",
    "        self.modeltri = dict()\n",
    "        self.modelbi = dict()\n",
    "        self.model = dict()\n",
    "        self.fimodel = dict()\n",
    "        self.fimodelbi = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        self.pro = list()\n",
    "        self.corpus = list()\n",
    "        self.poplist = list()\n",
    "    def inc_word(self, w):\n",
    "        opt = len(w)\n",
    "        if opt == 1:\n",
    "            if w in self.model:\n",
    "                self.model[w] += 1.0\n",
    "            else:\n",
    "                self.model[w] = 1.0\n",
    "        elif opt == 2:\n",
    "            if w in self.modelbi:\n",
    "                self.modelbi[w] += 1.0\n",
    "            else:\n",
    "                self.modelbi[w] = 1.0\n",
    "        elif opt == 3:\n",
    "            if w in self.modeltri:\n",
    "                self.modeltri[w] += 1.0\n",
    "            else:\n",
    "                self.modeltri[w] = 1.0\n",
    "        \n",
    "        \n",
    "    def fit_sentence(self, sentence):\n",
    "        unigrams = list(ngrams(sentence, 1, pad_left = False, pad_right=False))\n",
    "        bigrams = list(ngrams(sentence, 2, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = list(ngrams(sentence, 3, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = trigrams[:len(trigrams)-1]\n",
    "        for w in unigrams:\n",
    "            self.inc_word(('END_OF_SENTENCE',))\n",
    "            self.inc_word(w)\n",
    "        for w in bigrams:\n",
    "            self.inc_word(w)\n",
    "        for w in trigrams:\n",
    "            self.inc_word(w)\n",
    "       \n",
    "    def norm(self,corpus):\n",
    "        \"\"\"Normalize and convert to log2-probs.\"\"\"\n",
    "        v = 0.01*len(self.model.keys())\n",
    "        binum = 0\n",
    "        for k in self.fimodelbi:\n",
    "            if \"END_OF_SENTENCE\" not in k:\n",
    "                binum += 1\n",
    "        for word in self.fimodel:\n",
    "        #for word in self.modeltri:\n",
    "            if word == ('UNK',):\n",
    "                tot = 0\n",
    "                for w in self.model:\n",
    "                    tot += self.model[w]\n",
    "                print(\"tot is \", tot, \"v is\",v)\n",
    "                self.fimodel[word] = log(0.1* self.fimodel[word], 2) - log(v+tot, 2)\n",
    "\n",
    "            elif (word[0],word[1]) == ('*','*') :\n",
    "                self.fimodel[word] = log(0.01+self.fimodel[word],2)-log(v+len(corpus),2)\n",
    "            else:\n",
    "                self.fimodel[word] = log(0.01+self.fimodel[word], 2) -log(v+self.fimodelbi[(word[0],word[1])],2) \n",
    "                \n",
    "   \n",
    "    def cond_logprob(self, word, previous):\n",
    "        uni = self.model.keys()\n",
    "        word = list(word)\n",
    "        for i in range(0,len(word)):\n",
    "            if (word[i] in self.poplist) or ((word[i],) not in uni):\n",
    "                word[i] = 'UNK'\n",
    "        word = tuple(word)\n",
    "        if word in self.fimodel:\n",
    "            return self.fimodel[word]\n",
    "  \n",
    "        else:\n",
    "            v = len(self.model.keys())\n",
    "            return self.fimodel[('UNK',)]\n",
    "\n",
    "    def vocab(self):\n",
    "        return self.fimodel.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "# Python 3 backwards compatibility tricks\n",
    "if sys.version_info.major > 2:\n",
    "\n",
    "    def xrange(*args, **kwargs):\n",
    "        return iter(range(*args, **kwargs))\n",
    "\n",
    "    def unicode(*args, **kwargs):\n",
    "        return str(*args, **kwargs)\n",
    "\n",
    "\n",
    "def textToTokens(text):\n",
    "    \"\"\"Converts input string to a corpus of tokenized sentences.\n",
    "\n",
    "    Assumes that the sentences are divided by newlines (but will ignore empty sentences).\n",
    "    You can use this to try out your own datasets, but is not needed for reading the homework data.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    sents = text.split(\"\\n\")\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit(sents)\n",
    "    tokenizer = count_vect.build_tokenizer()\n",
    "    for s in sents:\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            corpus.append(toks)\n",
    "    return corpus\n",
    "\n",
    "def file_splitter(filename, seed = 0, train_prop = 0.7, dev_prop = 0.15,\n",
    "    test_prop = 0.15):\n",
    "    \"\"\"Splits the lines of a file into 3 output files.\"\"\"\n",
    "    import random\n",
    "    rnd = random.Random(seed)\n",
    "    basename = filename[:-4]\n",
    "    train_file = open(basename + \".train.txt\", \"w\")\n",
    "    test_file = open(basename + \".test.txt\", \"w\")\n",
    "    dev_file = open(basename + \".dev.txt\", \"w\")\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            p = rnd.random()\n",
    "            if p < train_prop:\n",
    "                train_file.write(l)\n",
    "            elif p < train_prop + dev_prop:\n",
    "                dev_file.write(l)\n",
    "            else:\n",
    "                test_file.write(l)\n",
    "    train_file.close()\n",
    "    test_file.close()\n",
    "    dev_file.close()\n",
    "\n",
    "def read_texts(tarfname, dname):\n",
    "    \"\"\"Read the data from the homework data file.\n",
    "\n",
    "    Given the location of the data archive file and the name of the\n",
    "    dataset (one of brown, reuters, or gutenberg), this returns a\n",
    "    data object containing train, test, and dev data. Each is a list\n",
    "    of sentences, where each sentence is a sequence of tokens.\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\", errors = 'replace')\n",
    "    for member in tar.getmembers():\n",
    "        if dname in member.name and ('train.txt') in member.name:\n",
    "            print('\\ttrain: %s'%(member.name))\n",
    "            train_txt = unicode(tar.extractfile(member).read(), errors='replace')\n",
    "        elif dname in member.name and ('test.txt') in member.name:\n",
    "            print('\\ttest: %s'%(member.name))\n",
    "            test_txt = unicode(tar.extractfile(member).read(), errors='replace')\n",
    "        elif dname in member.name and ('dev.txt') in member.name:\n",
    "            print('\\tdev: %s'%(member.name))\n",
    "            dev_txt = unicode(tar.extractfile(member).read(), errors='replace')\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit(train_txt.split(\"\\n\"))\n",
    "    tokenizer = count_vect.build_tokenizer()\n",
    "    class Data: pass\n",
    "    data = Data()\n",
    "    data.train = []\n",
    "    for s in train_txt.split(\"\\n\"):\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            data.train.append(toks)\n",
    "    data.test = []\n",
    "    for s in test_txt.split(\"\\n\"):\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            data.test.append(toks)\n",
    "    data.dev = []\n",
    "    for s in dev_txt.split(\"\\n\"):\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            data.dev.append(toks)\n",
    "    print(dname,\" read.\", \"train:\", len(data.train), \"dev:\", len(data.dev), \"test:\", len(data.test))\n",
    "    return data\n",
    "\n",
    "def learn_trigram(data, verbose=True):\n",
    "    \"\"\"Learns a trigram model from data.train.\n",
    "\n",
    "    It also evaluates the model on data.dev and data.test, along with generating\n",
    "    some sample sentences from the model.\n",
    "    \"\"\"\n",
    "    #from lm import Trigram\n",
    "    trigram = Trigram()\n",
    "    trigram.fit_corpus(data.train)\n",
    "    sumpro = 0;\n",
    "    #for w in trigram.model:\n",
    "     #   sumpro += 2 ** (trigram.model[w])\n",
    "    \n",
    "    #for w in trigram.modeltri:\n",
    "    #    sumpro += 2 ** (trigram.modeltri[w])\n",
    "    #for k in trigram.modelbi:\n",
    "    #    if \"END_OF_SENTENCE\" not in k:\n",
    "    #        binum += 1\n",
    "            \n",
    "    #for w in trigram.fimodel:\n",
    "     #   sumpro += 2 ** (trigram.fimodel[w])\n",
    "   \n",
    "    #print(\"sum of p(w):\", sumpro)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"vocab:\", len(trigram.vocab()))\n",
    "        # evaluate on train, test, and dev\n",
    "        print(\"train:\", trigram.perplexity(data.train))\n",
    "        print(\"dev  :\", trigram.perplexity(data.dev))\n",
    "        print(\"test :\", trigram.perplexity(data.test))\n",
    "        sampler = Sampler(trigram)\n",
    "        print(\"sample 1: \", \" \".join(str(x) for x in sampler.sample_sentence([])))\n",
    "        print(\"sample 2: \", \" \".join(str(x) for x in sampler.sample_sentence([])))\n",
    "    \n",
    "    return trigram\n",
    "\n",
    "def print_table(table, row_names, col_names, latex_file = None):\n",
    "    \"\"\"Pretty prints the table given the table, and row and col names.\n",
    "\n",
    "    If a latex_file is provided (and tabulate is installed), it also writes a\n",
    "    file containing the LaTeX source of the table (which you can \\input into your report)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from tabulate import tabulate\n",
    "        row_format =\"{:>15} \" * (len(col_names) + 1)\n",
    "        rows = map(lambda rt: [rt[0]] + rt[1], zip(row_names,table.tolist()))\n",
    "\n",
    "        print(tabulate(rows, headers = [\"\"] + col_names))\n",
    "        if latex_file is not None:\n",
    "            latex_str = tabulate(rows, headers = [\"\"] + col_names, tablefmt=\"latex\")\n",
    "            with open(latex_file, 'w') as f:\n",
    "                f.write(latex_str)\n",
    "                f.close()\n",
    "    except ImportError as e:\n",
    "        for row_name, row in zip(row_names, table):\n",
    "            print(row_format.format(row_name, *row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total UNK is  29\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "0 : UNK and UNK to UNK UNK of the UNK the UNK UNK UNK to UNK UNK with UNK the UNK UNK with UNK UNK of the UNK\n",
      "1 : UNK the UNK of the UNK UNK with UNK to UNK UNK UNK and UNK UNK to UNK to UNK UNK with UNK UNK and UNK UNK\n",
      "2 : with UNK UNK with UNK UNK with UNK UNK to UNK UNK of the UNK with UNK UNK to UNK UNK of the UNK with UNK UNK\n",
      "3 : of UNK and UNK the UNK of the UNK UNK to UNK UNK and UNK to UNK UNK UNK and UNK UNK UNK UNK to UNK UNK\n",
      "4 : UNK with UNK UNK and UNK with UNK UNK with UNK UNK to UNK UNK of the UNK of UNK and UNK and UNK UNK UNK the\n"
     ]
    }
   ],
   "source": [
    "#!/bin/python\n",
    "\n",
    "#from lm import LangModel\n",
    "import random\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "class Sampler:\n",
    "\n",
    "    def __init__(self, lm, temp = 0.6):\n",
    "        \"\"\"Sampler for a given language model.\n",
    "\n",
    "        Supports the use of temperature, i.e. how peaky we want to treat the\n",
    "        distribution as. Temperature of 1 means no change, temperature <1 means\n",
    "        less randomness (samples high probability words even more), and temp>1\n",
    "        means more randomness (samples low prob words more than otherwise). See\n",
    "        simulated annealing for what this means.\n",
    "        \"\"\"\n",
    "        self.lm = lm\n",
    "        self.rnd = random.Random()\n",
    "        self.temp = temp\n",
    "\n",
    "    def sample_sentence(self, prefix = [], max_length = 8):\n",
    "        \"\"\"Sample a random sentence (list of words) from the language model.\n",
    "\n",
    "        Samples words till either EOS symbol is sampled or max_length is reached.\n",
    "        Does not make any assumptions about the length of the context.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sent = prefix\n",
    "        word = self.sample_next(sent, False)\n",
    "        while i <= max_length and \"END_OF_SENTENCE\" not in word:\n",
    "            for j in range(0,len(word)):\n",
    "                sent.append(word[j])\n",
    "            word = self.sample_next(sent)\n",
    "            i += 1\n",
    "        return sent\n",
    "\n",
    "    def sample_next(self, prev, incl_eos = True):\n",
    "        \"\"\"Samples a single word from context.\n",
    "\n",
    "        Can be useful to debug the model, for example if you have a bigram model,\n",
    "        and know the probability of X-Y should be really high, you can run\n",
    "        sample_next([Y]) to see how often X get generated.\n",
    "\n",
    "        incl_eos determines whether the space of words should include EOS or not.\n",
    "        \"\"\"\n",
    "        wps = []\n",
    "        tot = -np.inf # this is the log (total mass)\n",
    "        for w in self.lm.vocab():\n",
    "            if not incl_eos and \"END_OF_SENTENCE\" in w:\n",
    "                continue\n",
    "            if w[0] == '*':\n",
    "                continue\n",
    "            lp = self.lm.cond_logprob(w, prev)\n",
    "            wps.append([w, lp/self.temp])\n",
    "            tot = np.logaddexp2(lp/self.temp, tot)\n",
    "        p = self.rnd.random()\n",
    "        word = self.rnd.choice(wps)[0]\n",
    "        s = -np.inf # running mass\n",
    "        for w,lp in wps:\n",
    "            s = np.logaddexp2(s, lp)\n",
    "            if p < pow(2, s-tot):\n",
    "                word = w\n",
    "                break\n",
    "        return word\n",
    "\n",
    "trigram = Trigram()\n",
    "corpus = [\n",
    "    data.train[0],data.train[1],data.train[0],data.train[0]\n",
    "]\n",
    "trigram.fit_corpus(corpus)\n",
    "sampler = Sampler(trigram)\n",
    "for i in xrange(5):\n",
    "    print(i, \":\", \" \".join(str(x) for x in sampler.sample_sentence([])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "\ttest: brown/brown.test.txt\n",
      "\ttrain: brown/brown.train.txt\n",
      "\tdev: brown/brown.dev.txt\n",
      "brown  read. train: 39802 dev: 8437 test: 8533\n",
      "total UNK is  14349\n",
      "sum of p(w): 1.0000000000001779\n",
      "vocab: 17227\n",
      "train: 886.7326210620298\n",
      "dev  : 661.3054058443886\n",
      "test : 658.6142906338957\n",
      "sample 1:  the once UNK and the is the the When as the UNK each in of and to the of on of two time between and The in UNK that the of\n",
      "sample 2:  in with to of working of of the by the the UNK UNK serene in all one few UNK of UNK UNK the UNK UNK of to and is of first\n",
      "-----------------------\n",
      "reuters\n",
      "\tdev: reuters/reuters.dev.txt\n",
      "\ttest: reuters/reuters.test.txt\n",
      "\ttrain: reuters/reuters.train.txt\n",
      "reuters  read. train: 38169 dev: 8082 test: 8214\n",
      "total UNK is  10648\n",
      "sum of p(w): 0.9999999999998426\n",
      "vocab: 15410\n",
      "train: 1016.4380307825963\n",
      "dev  : 859.134696374125\n",
      "test : 856.5198628865861\n",
      "sample 1:  UNK not of liquids on of the the the the said UNK government with the and and UNK mln in UNK and non did vs lt Inc the UNK from 25\n",
      "sample 2:  UNK said one UNK three April the 920 said in and and when Group UNK in mln vs the the the of UNK of its said will said that also of\n",
      "-----------------------\n",
      "gutenberg\n",
      "\tdev: gutenberg/gutenberg.dev.txt\n",
      "\ttrain: gutenberg/gutenberg.train.txt\n",
      "\ttest: gutenberg/gutenberg.test.txt\n",
      "gutenberg  read. train: 68740 dev: 14729 test: 14826\n",
      "total UNK is  7002\n",
      "sum of p(w): 1.0000000000000435\n",
      "vocab: 12439\n",
      "train: 509.5952721508509\n",
      "dev  : 507.96308405015736\n",
      "test : 515.0466190496704\n",
      "sample 1:  the of hast and of for hath of in the the be is the the the and to of 14 the ye the that the shalt to and the is of\n",
      "sample 2:  the of the more and it God Campbell UNK at of and the 29 brought the people you thee he to do For of is the to and the the to\n",
      "-------------------------------\n",
      "x train\n",
      "             brown    reuters    gutenberg\n",
      "---------  -------  ---------  -----------\n",
      "brown      736.265    598.823      543.101\n",
      "reuters    572.381    916.936      495.611\n",
      "gutenberg  465.843    466.315      523.656\n",
      "-------------------------------\n",
      "x dev\n",
      "             brown    reuters    gutenberg\n",
      "---------  -------  ---------  -----------\n",
      "brown      661.305    596.061      539.588\n",
      "reuters    568.179    859.135      499.483\n",
      "gutenberg  465.25     467.371      507.963\n",
      "-------------------------------\n",
      "x test\n",
      "             brown    reuters    gutenberg\n",
      "---------  -------  ---------  -----------\n",
      "brown      658.614    595.189      545.026\n",
      "reuters    572.652    856.52       495.366\n",
      "gutenberg  469.235    466.88       515.047\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "dnames = [ \"brown\",\"reuters\", \"gutenberg\"]\n",
    "datas = []\n",
    "models = []\n",
    "# Learn the models for each of the domains, and evaluate it\n",
    "for dname in dnames:\n",
    "    print(\"-----------------------\")\n",
    "    print(dname)\n",
    "    data = read_texts(\"data/corpora.tar.gz\", dname)\n",
    "    datas.append(data)\n",
    "    model = learn_trigram(data)\n",
    "    models.append(model)\n",
    "# compute the perplexity of all pairs\n",
    "n = len(dnames)\n",
    "perp_dev = np.zeros((n,n))\n",
    "perp_test = np.zeros((n,n))\n",
    "perp_train = np.zeros((n,n))\n",
    "for i in xrange(n):\n",
    "    for j in xrange(n):\n",
    "        perp_dev[i][j] = models[i].perplexity(datas[j].dev)\n",
    "        perp_test[i][j] = models[i].perplexity(datas[j].test)\n",
    "        perp_train[i][j] = models[i].perplexity(datas[j].train)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"x train\")\n",
    "print_table(perp_train, dnames, dnames, \"table-train.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x dev\")\n",
    "print_table(perp_dev, dnames, dnames, \"table-dev.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x test\")\n",
    "print_table(perp_test, dnames, dnames, \"table-test.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Trigram with Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "\ttest: brown/brown.test.txt\n",
      "\ttrain: brown/brown.train.txt\n",
      "\tdev: brown/brown.dev.txt\n",
      "brown  read. train: 39802 dev: 8437 test: 8533\n",
      "total UNK is  17763\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "tot is  1387366.0 v is 417.46000000000004\n",
      "vocab: 578551\n",
      "--------- [] ------------\n",
      "train: 372.94952566479816\n",
      "--------- [] ------------\n",
      "dev  : 569.4867873408967\n",
      "--------- [] ------------\n",
      "test : 570.1312713742814\n",
      "-----------------------\n",
      "reuters\n",
      "\tdev: reuters/reuters.dev.txt\n",
      "\ttest: reuters/reuters.test.txt\n",
      "\ttrain: reuters/reuters.train.txt\n",
      "reuters  read. train: 38169 dev: 8082 test: 8214\n",
      "total UNK is  14248\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "tot is  1930106.0 v is 360.37\n",
      "vocab: 628353\n",
      "--------- [] ------------\n",
      "train: 208.82598428586292\n",
      "--------- [] ------------\n",
      "dev  : 426.0229991514343\n",
      "--------- [] ------------\n",
      "test : 429.4876318687648\n",
      "-----------------------\n",
      "gutenberg\n",
      "\tdev: gutenberg/gutenberg.dev.txt\n",
      "\ttrain: gutenberg/gutenberg.train.txt\n",
      "\ttest: gutenberg/gutenberg.test.txt\n",
      "gutenberg  read. train: 68740 dev: 14729 test: 14826\n",
      "total UNK is  18319\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "tot is  2942066.0 v is 438.35\n",
      "vocab: 1021281\n",
      "--------- [] ------------\n",
      "train: 301.2156483569697\n",
      "--------- [] ------------\n",
      "dev  : 648.5064054175745\n",
      "--------- [] ------------\n",
      "test : 651.2823800123917\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "--------- [] ------------\n",
      "-------------------------------\n",
      "x train\n",
      "             brown    reuters    gutenberg\n",
      "---------  -------  ---------  -----------\n",
      "brown      372.95     444.897      560.483\n",
      "reuters    811.982    208.826      801.28\n",
      "gutenberg  880.889    750.255      301.216\n",
      "-------------------------------\n",
      "x dev\n",
      "             brown    reuters    gutenberg\n",
      "---------  -------  ---------  -----------\n",
      "brown      569.487    447.035      558.551\n",
      "reuters    811.045    426.023      801.52\n",
      "gutenberg  879.285    757.544      648.506\n",
      "-------------------------------\n",
      "x test\n",
      "             brown    reuters    gutenberg\n",
      "---------  -------  ---------  -----------\n",
      "brown      570.131    444.758      560.007\n",
      "reuters    814.587    429.488      796.455\n",
      "gutenberg  876.699    744.663      651.282\n"
     ]
    }
   ],
   "source": [
    "dnames = [ \"brown\",\"reuters\", \"gutenberg\"]\n",
    "datas = []\n",
    "models = []\n",
    "# Learn the models for each of the domains, and evaluate it\n",
    "for dname in dnames:\n",
    "    print(\"-----------------------\")\n",
    "    print(dname)\n",
    "    data = read_texts(\"data/corpora.tar.gz\", dname)\n",
    "    datas.append(data)\n",
    "    model = learn_trigram(data)\n",
    "    models.append(model)\n",
    "# compute the perplexity of all pairs\n",
    "n = len(dnames)\n",
    "perp_dev = np.zeros((n,n))\n",
    "perp_test = np.zeros((n,n))\n",
    "perp_train = np.zeros((n,n))\n",
    "for i in xrange(n):\n",
    "    for j in xrange(n):\n",
    "        perp_dev[i][j] = models[i].perplexity(datas[j].dev)\n",
    "        perp_test[i][j] = models[i].perplexity(datas[j].test)\n",
    "        perp_train[i][j] = models[i].perplexity(datas[j].train)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"x train\")\n",
    "print_table(perp_train, dnames, dnames, \"table-train.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x dev\")\n",
    "print_table(perp_dev, dnames, dnames, \"table-dev.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x test\")\n",
    "print_table(perp_test, dnames, dnames, \"table-test.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "\ttest: brown/brown.test.txt\n",
      "\ttrain: brown/brown.train.txt\n",
      "\tdev: brown/brown.dev.txt\n",
      "brown  read. train: 39802 dev: 8437 test: 8533\n",
      "total UNK is  12016\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "-----------------------\n",
      "reuters\n",
      "\tdev: reuters/reuters.dev.txt\n",
      "\ttest: reuters/reuters.test.txt\n",
      "\ttrain: reuters/reuters.train.txt\n",
      "reuters  read. train: 38169 dev: 8082 test: 8214\n",
      "total UNK is  8865\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "-----------------------\n",
      "gutenberg\n",
      "\tdev: gutenberg/gutenberg.dev.txt\n",
      "\ttrain: gutenberg/gutenberg.train.txt\n",
      "\ttest: gutenberg/gutenberg.test.txt\n",
      "gutenberg  read. train: 68740 dev: 14729 test: 14826\n",
      "total UNK is  5753\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n"
     ]
    }
   ],
   "source": [
    "dnames = [ \"brown\",\"reuters\", \"gutenberg\"]\n",
    "datas = []\n",
    "models = []\n",
    "# Learn the models for each of the domains, and evaluate it\n",
    "for dname in dnames:\n",
    "    print(\"-----------------------\")\n",
    "    print(dname)\n",
    "    data = read_texts(\"data/corpora.tar.gz\", dname)\n",
    "    datas.append(data)\n",
    "    model = learn_trigram(data)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('*', 'Dr', 'Bonnor'), ('up', 'with', 'the'), ('*', 'Miss', 'Schwarzkopf'), ('the', 'spirit', 'world'), ('that', 'no', 'matter'), ('and', 'at', 'times'), ('should', 'not', 'perish'), ('is', 'only', 'one'), ('see', 'the', 'light'), ('take', 'the', 'sacred'), ('New', 'England', 'Congregational'), ('New', 'England', 'theology'), ('of', 'three', 'hundred'), ('UNK', 'them', 'and'), ('and', 'had', 'the'), ('the', 'ground', 'and'), ('of', 'many', 'factors'), ('to', 'show', 'your'), ('to', 'show', 'the'), ('the', 'East', 'END_OF_SENTENCE'), ('power', 'of', 'UNK'), ('as', 'many', 'other'), ('UNK', 'them', 'to'), ('with', 'him', 'for'), ('two', 'years', 'in'), ('the', 'farm', 'and'), ('UNK', 'its', 'UNK'), ('because', 'he', 'knew'), ('they', 'are', 'UNK'), ('was', 'UNK', 'the'), ('they', 'are', 'not'), ('they', 'are', 'the'), ('they', 'are', 'in'), ('was', 'UNK', 'with'), ('000', '000', 'the'), ('University', 'of', 'Oklahoma'), ('He', 'has', 'served'), ('the', 'State', 'Board'), ('to', 'meet', 'END_OF_SENTENCE'), ('up', 'for', 'UNK'), ('that', 'would', 'be'), ('University', 'of', 'Chicago'), ('the', 'center', 'END_OF_SENTENCE'), ('failed', 'to', 'reach'), ('to', 'its', 'UNK'), ('who', 'UNK', 'that'), ('of', 'men', 'END_OF_SENTENCE'), ('difficult', 'to', 'see'), ('*', 'New', 'Mexico'), ('had', 'no', 'chance'), ('what', 'it', 'can'), ('failed', 'to', 'do'), ('or', 'not', 'and'), ('*', 'New', 'Jersey'), ('to', 'one', 'fourth'), ('or', 'not', 'there'), ('to', 'one', 'another'), ('the', 'war', 'and'), ('what', 'it', 'was'), ('know', 'that', 'my'), ('that', 'would', 'UNK'), ('For', 'example', 'in'), ('the', 'water', 'at'), ('know', 'that', 'they'), ('who', 'UNK', 'in'), ('For', 'example', 'END_OF_SENTENCE'), ('or', 'not', 'to'), ('or', 'not', 'END_OF_SENTENCE'), ('of', 'American', 'life'), ('of', 'American', 'UNK'), ('know', 'that', 'the'), ('that', 'all', 'of'), ('He', 'has', 'been'), ('this', 'time', 'with'), ('he', 'can', 'END_OF_SENTENCE'), ('or', 'not', 'is'), ('what', 'it', 'does'), ('failed', 'to', 'make'), ('is', 'good', 'and'), ('for', 'some', 'people'), ('the', 'heart', 'is'), ('University', 'of', 'California'), ('which', 'we', 'find'), ('be', 'used', 'but'), ('know', 'that', 'we'), ('difficult', 'to', 'UNK'), ('what', 'he', 'did'), ('when', 'they', 'have'), ('they', 'would', 'require'), ('to', 'one', 'engine'), ('point', 'of', 'impact'), ('the', 'heart', 'itself'), ('*', 'New', 'rule'), ('were', 'not', 'UNK'), ('what', 'he', 'thought'), ('the', 'kind', 'and'), ('be', 'used', 'END_OF_SENTENCE'), ('out', 'and', 'rustle'), ('in', 'the', 'past'), ('*', 'He', 'can')]\n"
     ]
    }
   ],
   "source": [
    "v1 = models[0].fimodel\n",
    "sortv1 = sorted(v1.items(), key=lambda kv: kv[1])\n",
    "print([w[0] for w in sortv1[len(v1)-10000:len(v1)-9900]])\n",
    "v2 = models[1].fimodel\n",
    "sortv2 = sorted(v2.items(), key=lambda kv: kv[1])\n",
    "#print([w[0] for w in sortv2[len(v2)-20:]])\n",
    "v3 = models[2].fimodel\n",
    "sortv3 = sorted(v3.items(), key=lambda kv: kv[1])\n",
    "#print([w[0] for w in sortv3[len(v3)-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-65.335927830711"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].logprob_sentence(['*','If','you','is','one','of','member','of','the','United','States'],datas[0].train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-94.83004760995267"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].logprob_sentence(['*','Dr','Bonner','is','one','of','member','of','University','of','Oklahoma','in','the','past'],datas[0].train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].logprob_sentence(['*','If','you','is','one','of','member','of','the','United','States'],datas[0].train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('through', 'the'), ('does', 'not'), ('may', 'be'), ('rather', 'than'), ('will', 'be'), ('kind', 'of'), ('per', 'cent'), ('from', 'the'), ('should', 'be'), ('during', 'the'), ('in', 'the'), ('part', 'of'), ('able', 'to'), ('of', 'the'), ('at', 'the'), ('It', 'is'), ('New', 'York'), ('on', 'the'), ('number', 'of'), ('United', 'States')]\n",
      "[('added', 'END_OF_SENTENCE'), ('4TH', 'QTR'), ('He', 'said'), ('sources', 'said'), ('lt', 'UNK'), ('QTR', 'NET'), ('mths', 'Shr'), ('compared', 'with'), ('expected', 'to'), ('due', 'to'), ('did', 'not'), ('United', 'States'), ('CORP', 'lt'), ('Nine', 'mths'), ('09', '87'), ('NET', 'Shr'), ('INC', 'lt'), ('1ST', 'QTR'), ('3RD', 'QTR'), ('Avg', 'shrs')]\n",
      "[('dare', 'say'), ('ark', 'of'), ('sort', 'of'), ('inhabitants', 'of'), ('obliged', 'to'), ('pray', 'thee'), ('round', 'about'), ('Frank', 'Churchill'), ('midst', 'of'), ('into', 'the'), ('Captain', 'Wentworth'), ('spake', 'unto'), ('out', 'of'), ('Oh', 'END_OF_SENTENCE'), ('sons', 'of'), ('tribe', 'of'), ('son', 'of'), ('able', 'to'), ('according', 'to'), ('children', 'of')]\n"
     ]
    }
   ],
   "source": [
    "v1 = models[0].fimodelbi\n",
    "sortv1 = sorted(v1.items(), key=lambda kv: kv[1])\n",
    "print([w[0] for w in sortv1[len(v1)-20:]])\n",
    "v2 = models[1].fimodelbi\n",
    "sortv2 = sorted(v2.items(), key=lambda kv: kv[1])\n",
    "print([w[0] for w in sortv2[len(v2)-20:]])\n",
    "v3 = models[2].fimodelbi\n",
    "sortv3 = sorted(v3.items(), key=lambda kv: kv[1])\n",
    "print([w[0] for w in sortv3[len(v3)-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import collections\n",
    "from math import log\n",
    "import sys\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Python 3 backwards compatibility tricks\n",
    "if sys.version_info.major > 2:\n",
    "\n",
    "    def xrange(*args, **kwargs):\n",
    "        return iter(range(*args, **kwargs))\n",
    "\n",
    "    def unicode(*args, **kwargs):\n",
    "        return str(*args, **kwargs)\n",
    "\n",
    "class LangModel:\n",
    "    def fit_corpus(self, corpus):\n",
    "        \"\"\"Learn the language model for the whole corpus.\n",
    "\n",
    "        The corpus consists of a list of sentences.\"\"\"\n",
    "        for s in corpus:\n",
    "            self.fit_sentence(s)\n",
    "         \n",
    "        vo = self.model.keys()\n",
    "        vobi = self.modelbi.keys()\n",
    "        votri = self.modeltri.keys()\n",
    "        for w in vo:\n",
    "            if self.model[w] == 1:\n",
    "                self.poplist.append(w[0])\n",
    "        self.model[('UNK',)] = len(self.poplist)\n",
    "        print(\"total UNK is \", len(self.poplist))\n",
    "        for w in self.poplist:\n",
    "            self.model.pop((w,))\n",
    "        for tri in votri:\n",
    "            old = tri\n",
    "            tri = list(tri)\n",
    "            for i in range(0,len(tri)):\n",
    "                if tri[i] in self.poplist:\n",
    "                    tri[i] = 'UNK'\n",
    "            tri = tuple(tri)\n",
    "            if tri in self.fimodel:\n",
    "                    self.fimodel[tri] += self.modeltri[old]\n",
    "            else:\n",
    "                    self.fimodel[tri] = self.modeltri[old]\n",
    "        print(\"finished processing tri with unk\")    \n",
    "        for bi in vobi:\n",
    "            oldbi = bi\n",
    "            bi = list(bi)\n",
    "            for i in range(0,len(bi)):\n",
    "                if bi[i] in self.poplist:\n",
    "                    bi[i] = 'UNK'\n",
    "            bi = tuple(bi)\n",
    "            if bi in self.fimodelbi:\n",
    "                self.fimodelbi[bi] += self.modelbi[oldbi]\n",
    "            else:\n",
    "                self.fimodelbi[bi] = self.modelbi[oldbi]   \n",
    "        print(\"finished processing bi with unk\")    \n",
    "        \n",
    "        self.norm(corpus)\n",
    "\n",
    "    def perplexity(self, corpus):\n",
    "        \"\"\"Computes the perplexity of the corpus by the model.\n",
    "\n",
    "        Assumes the model uses an EOS symbol at the end of each sentence.\n",
    "        \"\"\"\n",
    "        print('---------',self.pro,'------------')\n",
    "        return pow(2.0, self.entropy(corpus))\n",
    "\n",
    "    def entropy(self, corpus):\n",
    "        num_words = 0.0\n",
    "        sum_logprob = 0.0\n",
    "        for s in corpus:\n",
    "            num_words += len(s)  # for EOS\n",
    "            sum_logprob += self.logprob_sentence(s,corpus)\n",
    "        return -(1.0/num_words)*(sum_logprob)\n",
    "\n",
    "    def logprob_sentence(self, sentence,corpus):\n",
    "        p = 0.0\n",
    "        trigrams = list(ngrams(sentence, 3, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = trigrams[:len(trigrams)-1]\n",
    "        for i in xrange(len(sentence)):\n",
    "            p += self.cond_logprob(trigrams[i],trigrams[:i])\n",
    "        return p\n",
    "\n",
    "    # required, update the model when a sentence is observed\n",
    "    def fit_sentence(self, sentence): pass\n",
    "    # optional, if there are any post-training steps (such as normalizing probabilities)\n",
    "    def norm(self,corpus): pass\n",
    "    # required, return the log2 of the conditional prob of word, given previous words\n",
    "    def cond_logprob(self, word, previous): pass\n",
    "    # required, the list of words the language model suports (including EOS)\n",
    "    def vocab(self): pass\n",
    "\n",
    "class Trigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001):\n",
    "        self.modeltri = dict()\n",
    "        self.modelbi = dict()\n",
    "        self.model = dict()\n",
    "        self.fimodel = dict()\n",
    "        self.fimodelbi = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        self.pro = list()\n",
    "        self.corpus = list()\n",
    "        self.poplist = list()\n",
    "    \n",
    "    def inc_word(self, w):\n",
    "        opt = len(w)\n",
    "        if opt == 1:\n",
    "            if w in self.model:\n",
    "                self.model[w] += 1.0\n",
    "            else:\n",
    "                self.model[w] = 1.0\n",
    "        elif opt == 2:\n",
    "            if w in self.modelbi:\n",
    "                self.modelbi[w] += 1.0\n",
    "            else:\n",
    "                self.modelbi[w] = 1.0\n",
    "        elif opt == 3:\n",
    "            if w in self.modeltri:\n",
    "                self.modeltri[w] += 1.0\n",
    "            else:\n",
    "                self.modeltri[w] = 1.0\n",
    "        \n",
    "        \n",
    "    def fit_sentence(self, sentence):\n",
    "        unigrams = list(ngrams(sentence, 1, pad_left = False, pad_right=False))\n",
    "        bigrams = list(ngrams(sentence, 2, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = list(ngrams(sentence, 3, pad_left = True, left_pad_symbol= '*', pad_right=True,right_pad_symbol='END_OF_SENTENCE'))\n",
    "        trigrams = trigrams[:len(trigrams)-1]\n",
    "        for w in unigrams:\n",
    "            self.inc_word(('END_OF_SENTENCE',))\n",
    "            self.inc_word(w)\n",
    "        for w in bigrams:\n",
    "            self.inc_word(w)\n",
    "        for w in trigrams:\n",
    "            self.inc_word(w)\n",
    "       \n",
    "    def norm(self,corpus):\n",
    "        \"\"\"Normalize and convert to log2-probs.\"\"\"\n",
    "        #binum = 0\n",
    "        #for k in self.fimodelbi:\n",
    "         #   if \"END_OF_SENTENCE\" not in k:\n",
    "          #      binum += 1\n",
    "        v = 0.01*len(self.model.keys())\n",
    "\n",
    "        for word in self.fimodel:\n",
    "            if (word[0],word[1]) == ('*','*') :\n",
    "                self.fimodel[word] = log(0.01+self.fimodel[word],2)-log(v+len(corpus),2)\n",
    "        #        self.modeltri[word] = log(1+self.modeltri[word],2)-log(v+len(corpus),2)\n",
    "            else:\n",
    "                self.fimodel[word] = log(0.01+self.fimodel[word], 2) -log(v+self.fimodelbi[(word[0],word[1])],2) \n",
    "        #        self.modeltri[word] = log(1+self.modeltri[word], 2) -log(v+self.modelbi[(word[0],word[1])],2)\n",
    "        for word in self.fimodelbi:\n",
    "            if word[0] == '*':\n",
    "                self.fimodelbi[word] = log(0.01+self.fimodelbi[word],2)-log(v+len(corpus),2)\n",
    "            else:\n",
    "                self.fimodelbi[word] = log(0.01+self.fimodelbi[word], 2) -log(v+self.model[(word[0],)],2) \n",
    "        tot = 0\n",
    "        for w in self.model:\n",
    "                tot += self.model[w]\n",
    "        for word in self.model:\n",
    "            self.model[word] = log(0.01+self.model[word], 2) - log(v+tot, 2)\n",
    "            \n",
    "    def cond_logprob(self, word, previous):\n",
    "        uni = self.model.keys()\n",
    "        word = list(word)\n",
    "        for i in range(0,len(word)):\n",
    "            if (word[i] in self.poplist) or ((word[i],) not in uni):\n",
    "                word[i] = 'UNK'\n",
    "        word = tuple(word)\n",
    "        \n",
    "        if word in self.fimodel:\n",
    "            qsum = 0.8*self.fimodel[word]+0.1*self.fimodelbi[(word[1],word[2])]+0.1*self.model[(word[2],)]\n",
    "        elif (word[1],word[2]) in self.fimodelbi:\n",
    "            qsum = 0.8*self.fimodelbi[(word[1],word[2])]+0.2*self.model[(word[2],)]\n",
    "        #elif word[2] == 'UNK':\n",
    "         #   qsum = 0.01*self.model[(word[2],)]\n",
    "        else:\n",
    "            qsum = self.model[(word[2],)]\n",
    "        return qsum\n",
    "\n",
    "    def vocab(self):\n",
    "      #  return self.modeltri.keys()\n",
    "        return self.fimodel.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "\ttest: brown/brown.test.txt\n",
      "\ttrain: brown/brown.train.txt\n",
      "\tdev: brown/brown.dev.txt\n",
      "brown  read. train: 39802 dev: 8437 test: 8533\n",
      "total UNK is  12016\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "-----------------------\n",
      "reuters\n",
      "\tdev: reuters/reuters.dev.txt\n",
      "\ttest: reuters/reuters.test.txt\n",
      "\ttrain: reuters/reuters.train.txt\n",
      "reuters  read. train: 38169 dev: 8082 test: 8214\n",
      "total UNK is  8865\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n",
      "-----------------------\n",
      "gutenberg\n",
      "\tdev: gutenberg/gutenberg.dev.txt\n",
      "\ttrain: gutenberg/gutenberg.train.txt\n",
      "\ttest: gutenberg/gutenberg.test.txt\n",
      "gutenberg  read. train: 68740 dev: 14729 test: 14826\n",
      "total UNK is  5753\n",
      "finished processing tri with unk\n",
      "finished processing bi with unk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nn = len(dnames)\\nperp_dev = np.zeros((n,n))\\nperp_test = np.zeros((n,n))\\nperp_train = np.zeros((n,n))\\nfor i in xrange(n):\\n    for j in xrange(n):\\n        perp_dev[i][j] = modelssss[i].perplexity(datas[j].dev)\\n        perp_test[i][j] = modelssss[i].perplexity(datas[j].test)\\n        perp_train[i][j] = modelssss[i].perplexity(datas[j].train)\\n\\nprint(\"-------------------------------\")\\nprint(\"x train\")\\nprint_table(perp_train, dnames, dnames, \"table-train.tex\")\\nprint(\"-------------------------------\")\\nprint(\"x dev\")\\nprint_table(perp_dev, dnames, dnames, \"table-dev.tex\")\\nprint(\"-------------------------------\")\\nprint(\"x test\")\\nprint_table(perp_test, dnames, dnames, \"table-test.tex\")\\n'"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnames = [ \"brown\",\"reuters\", \"gutenberg\"]\n",
    "datassss = []\n",
    "modelssss = []\n",
    "# Learn the models for each of the domains, and evaluate it\n",
    "for dname in dnames:\n",
    "    print(\"-----------------------\")\n",
    "    print(dname)\n",
    "    data = read_texts(\"data/corpora.tar.gz\", dname)\n",
    "    datassss.append(data)\n",
    "    model = learn_trigram(data)\n",
    "    modelssss.append(model)\n",
    "# compute the perplexity of all pairs\n",
    "'''\n",
    "n = len(dnames)\n",
    "perp_dev = np.zeros((n,n))\n",
    "perp_test = np.zeros((n,n))\n",
    "perp_train = np.zeros((n,n))\n",
    "for i in xrange(n):\n",
    "    for j in xrange(n):\n",
    "        perp_dev[i][j] = modelssss[i].perplexity(datas[j].dev)\n",
    "        perp_test[i][j] = modelssss[i].perplexity(datas[j].test)\n",
    "        perp_train[i][j] = modelssss[i].perplexity(datas[j].train)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"x train\")\n",
    "print_table(perp_train, dnames, dnames, \"table-train.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x dev\")\n",
    "print_table(perp_dev, dnames, dnames, \"table-dev.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x test\")\n",
    "print_table(perp_test, dnames, dnames, \"table-test.tex\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
